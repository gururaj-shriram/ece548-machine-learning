\relax 
\providecommand\zref@newlabel[2]{}
\citation{kubat}
\citation{kubat}
\citation{scikit}
\citation{numpy}
\providecommand \oddpage@label [2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Task}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{1}}
\citation{ionosphere}
\citation{sun-kamel-wang}
\citation{cancer}
\@writefile{toc}{\contentsline {section}{\numberline {4}Ionosphere Dataset}{2}}
\citation{sun-kamel-wang}
\citation{musk}
\citation{sun-kamel-wang}
\bibstyle{plainurl}
\bibdata{bibi}
\bibcite{cancer}{1}
\@writefile{toc}{\contentsline {section}{\numberline {5}Cancer Dataset}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Musk Dataset}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{3}}
\bibcite{ionosphere}{2}
\bibcite{musk}{3}
\bibcite{kubat}{4}
\bibcite{numpy}{5}
\bibcite{scikit}{6}
\bibcite{sun-kamel-wang}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The error rate on the testing and training sets drop dramatically with the first 10 classifiers. Adaboost does slightly better than a single perceptron classifier and consistently worse than a decision tree. The testing set error generally decreases with more subclassifiers, approaching the decision tree's accuracy.}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The testing and training set errors here experience an even sharper relative decline. The dataset is noisy, leading the decision tree to do worse than Adaboost and the single perceptron. After hitting a minimum between 20 and 30 examples, the testing set error rate begins to climb back up again due to the noise in the data and bad examples being chosen to induce subclassifiers.}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The data here is especially noisy, causing the training set error rate to jump up and down erratically. The testing set error does not fare much better, reaching a minimum around 23 and climbing back up past the error rate of a single perceptron classifier. The decision tree also suffers due to the noise in the data.}}{7}}
