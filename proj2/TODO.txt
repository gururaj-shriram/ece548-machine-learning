Project description (Group #7): 

Implement the basic version of Adaboost. The number of voting classifiers is 
determined by a user-set constant. Another user-set constant specifies the number 
of examples in each training set, Ti. The weights of the individual classifiers are 
obtained with the help of perceptron learning. Apply the program task to some of
the benchmark domains from the UCI repository. Make observations about this
program's performance on different data. For each domain, plot a graph showing
how the overall accuracy of the resulting classifier depends on the number of
subclassifiers. Also, observe how the error rate on the training set and the error
rate on the testing set tend to converge with the growing number of classifiers.
------------------------------------------------------------------------------------
11/10 - update [by: jerry]

Some things Kubat has mentioned to me: 
   * datasets should have only two classes: "pos" versus "neg"
   * base-line learners should be linear classifiers 
   * everything to be implemented ourselves  

Presentations begin first day after thanksgiving break, 11/27. So probably we will 
present that friday, 12/1.
-----------------------------------------------------------------------------------  
11/12 - info on datasets [by: jerry]

1) default of credit card clients -- seems like a difficult dataset. on perceptron,
  error rate does not seem to go below 20%. will be interesting to see what happens
  after boosting. 
  --> see default.csv and default.names for info 

2) modified wine (for only 2 classes) -- this is only for testing purposes. I 
   modified the wine dataset to have only two classes to verify that perceptron
   is working. do not use this for graphs and the report, way too easy.  

3) ionosphere 

need more datasets! 
-----------------------------------------------------------------------------------  
11/22 - update [by: jerry]

I received this e-mail from the professor: 

"By the way. When generating the random subsets, you guys can perhaps benefit from 
a little programming trick explained in my book, Chapter 13, section 2, 
(pp.258-9in the first edition)."

This is in reference to the genetic algorithm chapter. In particular, he is talking
about the wheel of fortune in which individuals are chosen. It took awhile to figure
out what this had to do with Adaboost, but the idea is this: 

Recall that we were having trouble with getting the required number of examples
for each training subset. The wheel of fortune is an approach to solving this. 

Suppose we have 5 examples. The initial probabilities will then be 1 / 5 = 0.2 
Now imagine the wheel of fortune (pg. 310 in book), which is really a dart-board. 
Each of these 5 examples comprises a portion of this dart-board, in specific, 20%
of it. So ex1 will range from [0, 0.2), ex2 [0.2, 0.4), ex3 [0.4, 0.6), ..., 
ex5 [0.8, 1). 

Now we throw darts at this board and see where the dart lands. Say that it falls 
on 0.3. That means we select ex2 to add into the subset. We keep throwing darts 
until we've reached the desired number of examples for the training subset.    

The book mentions that F = SUM(Fi) where Fi is each individual's "fitness". 
In our case, Fi is each of the individual probabilities for the examples. And
this makes sense because 0.2 + 0.2 + 0.2 + 0.2 + 0.2 = 1, because all probabilities
must add up to 1. So the entire dartboard has probability of 1. 

I thought this was clever but it turns out that Guru had discovered numpy and numpy
takes care of all of the above with one line: 

np.random.choice(n_training, self.train_num, replace=False, p=prob_list) 

[https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html]

They may not implement their sampling the same way, but I think that this wheel of
fortune should be discussed in the report. 

Next, are some questions with the implementation. Please see comments in adaboost.py.
	- for weighted majority voting, what weight-update formula do we use 
	  for the classifiers? 
	- what termination criteria do we use for adaboost? Until we reach the desired
	  number of classifiers? or until we reach a threshold? 
	- in update_distribution(), do all probabilities get modified by beta_i? or only
	  those that are correctly classified? if so, why do we get better results when
	  we modify all probabilities? 
	- decision trees completely destroys these datasets. perceptron alone does pretty
	  terrible, on average. Adaboost is somewhere in between. Expected? 

I had to forego Guru's get_result_list() because I didn't realize at the time that
Adaboost does not use plain-voting. It uses a weighted majority voting, with another
pass to perceptron to decide the weight of the votes. 

I found two new datasets:
	- ionosphere
	- magic04

-----------------------------------------------------------------------------------  










