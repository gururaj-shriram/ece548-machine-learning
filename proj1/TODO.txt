Project description: (Group #5) 

k-NN classifiers: investigate the detrimental effect of irrelevant attributes. 
Method: create some absolutely clean datafile where you know all attributes are 
relevant; then, keep adding groups of randomly generated attributes. 
--------------------------------------------------------------------------------

This is what Kubat said in class; we have to prove that error rate increases 
with the number of irrelevant attributes

Error Rate
^
|     /
|    /
|   /
|  /
+-----------> # of irrelevant attributes


Start out with 2 good attributes and continually add bad ones (with RNG), 
showing how error rate increases with # of irrelevant attributes 

--------------------------------------------------------------------------------

Update from Jerry (10/13):

I believe I have finished the program so I would like to discuss the results, 
data, and what I have done. I have tried to comment the python script as 
much as possible. Here is what I did:

- There are two nearest neighbor classifiers at play here. The first one I 
  coded myself using the algorithm from the textbook. In the code, this is 
  the method nearest_neighbors_implementation(). If you see variables 
  prefixed with "ours", then this means I am referring to my own NN.   

  The second one comes from Scikit learn, a Python library. I use their 
  classifier in order to verify the validity of my own algorithm. In the 
  code, this is the method nearest_neighbors_scikit(). If you see variables
  prefixed with "scikit", then this means I am referring to Python's NN.  

- Given the dataset, I split it into a training set and testing set using a
  probability constant called PROBABILITY_TRAINING_SET, which is currently
  set to 0.7. This means that there is a 70% chance of an available example
  being placed into the training set. In contrast, there is a 30% chance
  of an example being placed into the testing set. So the split is 70-30. 

  The training set functions as the examples that are on the scene. An 
  example from the testing set functions as the test point, akin to
  what we saw in class, and we would like to see how it gets classified. 
  What it gets classified as is what I call a ``hypothesis''. 

  Again, visit the nearest_neighbors_implementation() method for my algorithm. 

  Then I employ random subsampling to verify that the accuracy/error rate
  that we get is reliable. Kubat has a good line in his book that this 
  can help remedy the problem of non-representative training sets, which
  can certainly happen given the random division of training and testing set
  (bottom of p. 11 in 2nd edition). See average_for_runs() method. 

- Performance is measured based on error rate. So I get the number of 
  mis-classified examples, and divide by total number in the testing set 
  to get the error rate. Then I do 1 - error_rate to get the accuracy rate.
  Accuracy rate is used throughout the rest of the program and is what gets 
  written to file for graphing the results. (thoughts?)

- NN is first run using our NN and scikit NN without any irrelevant attributes.
  Usually this gives around ~93% accuracy. Then irrelevant attributes are 
  added to the dataset in groups of two, until the desired number of groups
  is reached. That number is defined by the constant NUM_GROUPS, which 
  is set to 10, which means 10 * 2 = 20 irrelevant attributes.    

- Generating irrelevant attributes: To generate irrelevant attribute values,
  I get a normal distribution based on a constant RANDOM_DATA_MEAN and 
  a standard deviation RANDOM_DATA_STD. These will initially have small
  values since the mean is set to 10 and the STD is only 1. But as 
  groups of irrelevant attributes are added, I multiply the mean by the 
  current iteration. So for instance, if we want 10 groups and now we are 
  on 8 groups of irrelevant, then the mean will be centered at 
  10 * (8 + 1) = 90, because the 1 is added for fun, and the STD will 
  be 3 + 8 = 11. The point here is to make as much noise as possible such that
  it will influence the data. 

  And the point of the normal is to ensure the data falls within some range,
  albeit a range whose values have no meaning (think of chef-shoe-size), versus 
  using a random number generator. Even though the attribute is irrelevant, it 
  should still "act" like any other attribute.  

  See add_a_new_dimension(). 

- The above process is done for 1-NN, 3-NN, 5-NN, ..., 9-NN. There is a 
  constant MAX_K that can be changed to adjust what NN to go up to. 

- This above point needs discussion. Because our datasets have multiple classes,
  it is possible for voting neighbors to tie. Right now there is nothing being
  done to break ties because that wasn't the assignment (again, thoughts?). What
  is done is that I have a list of all distances and then I sort it. So if 
  there are 2 votes for tiger and 2 votes for bunny, whichever Python decides to
  place at the front of the list after sorting is what the algorithm takes. So
  the solution is randomness right now. 

- The script outputs two text files: sci_accuracy.txt and our_accuracy.txt. Each 
  row in the file is the output for a k-NN. So the first row is 1-NN. Each column
  represents the performance for that round of irrelevant attributes. 
  
  These data can then be imported to Excel as csv file. I did that and pushed a 
  data.xslx file to the report directory. 

These are some questions I have regarding the output...

Questions:

- What to do about contesting neighbors in the case of 3-NN or 5-NN?
- Why is it that my naive algorithm can do better than Python's k-NN? 
- Why is there a difference in performance between the iris and animals dataset? 
- Is there a reason why 7-NN shows the strongest performance out of all the k-NN? Not 
  only that, it seems to be the best for *both* datasets. Why?
- Which k-NN's should we put on our graphs? I picked 1-NN and 7-NN. 

That's it for now. 

--------------------------------------------------------------------------------

Update from Guru (10/14):

- Modified the logic for choosing which K-NN to train on to prevent having 
  "ties" (where one class may have the same number of "votes" as another class)
- Added new data to reflect the above point
- Added functionality to create a color map which plots decision bound areas
  for each class. The map plots the training data along with colors of each 
  class "area" with a given K-NN. Currently, the script outputs a png file for 
  each K-NN and for 0, NUM_GROUPS / 2, and NUM_GROUPS irrelevant attributes 
  (since a plot for each iteration was too computationally intensive).

  Outputs are in the form DATASET-K-NN-M-irrelevant.png

  See examples at reports/classifier\ color\ maps